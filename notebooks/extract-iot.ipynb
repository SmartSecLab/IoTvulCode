{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries on CVE records for the extraction of IoT related referenced repositories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import json \n",
    "import ast\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import requests\n",
    "import tempfile\n",
    "from io import BytesIO, StringIO\n",
    "from zipfile import ZipFile\n",
    "from guesslang import Guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cve-records.csv')\n",
    "des_str = df['description'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Query: \n",
    "\"Internet of Things\" OR \"IoT\" OR \"Industry 4.0\" OR \"smart cities\" OR \"smart city\"OR \"smart contract\" OR \"manufacturing\" OR \"energy\" OR \"supply chain\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(des_str):\n",
    "    if des_str!=None or des_str!='':\n",
    "        des_arr_dict = ast.literal_eval(des_str)\n",
    "        des_cve = \"\"    #description of a CVE-> 'value' from array of dict.\n",
    "\n",
    "        for dic in des_arr_dict:\n",
    "            des_cve = des_cve + dic['value']\n",
    "        return des_cve\n",
    "        \n",
    "    else:\n",
    "        print('Empty description for CVE: ')\n",
    "        return 0\n",
    "\n",
    "def get_iot_cves(df):\n",
    "    iot_set = [\"Internet of Things\", \"IoT\", \"Industry 4.0\", \n",
    "                \"smart cities\", \"smart city\", \"smart contract\", \n",
    "                \"manufacturing\", \"energy\", \"supply chain\", \"orange pi\", \"banana pi\", \"arduino\"]\n",
    "    iot_cves = []\n",
    "\n",
    "    for row in range(len(df)):\n",
    "        des_cve = get_description(df['description'][row])\n",
    "        \n",
    "        # print if they are IoT related descriptions\n",
    "        for x in iot_set:\n",
    "            if x.lower() in des_cve.lower():\n",
    "                # print(des_cve)\n",
    "                # print(df['cve_id'][row])\n",
    "                iot_cves.append(df['cve_id'][row])\n",
    "                # print(df['reference_json'][row])\n",
    "                # print('\\n')\n",
    "    return iot_cves\n",
    "\n",
    "iot_cves = get_iot_cves(df)\n",
    "print('count_cves:', len(iot_cves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iot = df[df.cve_id.isin(iot_cves)]\n",
    "len(df_iot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iot_vcs = ['github', 'bitbucket', 'gitlab']\n",
    "vcs_list = []\n",
    "\n",
    "for ref_str in df_iot.reference_json:\n",
    "    url_dict  = ast.literal_eval(ref_str)\n",
    "    \n",
    "    if len(url_dict) > 0:\n",
    "        for ref in url_dict:\n",
    "            vcs_list.append(ref['url'])     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vulnerabilty reporting databases and number of their occurances in CVEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_freq = collections.Counter(url_heads)\n",
    "df_url = pd.DataFrame(url_freq.items(), columns=['urls', 'count'])\n",
    "df_url = df_url.sort_values(by=['count'], ascending=False)\n",
    "df_url.to_csv('../result/top-databases.csv', index=False, sep=';')\n",
    "df_url.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Infer output (report.json) file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import subprocess as sub\n",
    "import time \n",
    "\n",
    "############################ Applying infer tool ############################\n",
    "\n",
    "def json2df(file) -> pd.DataFrame:\n",
    "    df = pd.DataFrame()\n",
    "    with open(file) as f:\n",
    "        data = json.load(f)\n",
    "        df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def apply_infer(fname) -> pd.DataFrame:\n",
    "    \"\"\"find flaws in the file using infer tool\"\"\"\n",
    "    infer_dir = 'infer-output'\n",
    "    compiler = 'gcc'\n",
    "\n",
    "    cmd = f\"infer run --results-dir {infer_dir} -- {compiler} -c \"\n",
    "    out_file = f\"{infer_dir}/report.json\" # output file generated by infer tool\n",
    "\n",
    "    df = pd.DataFrame() # dataframe to store the results\n",
    "\n",
    "    if os.path.isfile(fname):\n",
    "        cmd =  cmd + fname\n",
    "        process = sub.Popen(\n",
    "            cmd,\n",
    "            shell=True,\n",
    "            stdout=sub.PIPE,\n",
    "        )\n",
    "        process.wait() # wait for the process to finish\n",
    "        \n",
    "        # check if the output file is generated\n",
    "        if os.path.isfile(out_file):\n",
    "            df = json2df(out_file)\n",
    "        if len(df)!=0:\n",
    "            df[\"tool\"] = \"infer\"\n",
    "        df = df.reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        print(f'Invalid command for infer tool! \\\n",
    "            \\nPlease check the command again! \\ncommand: {cmd}')\n",
    "    return df\n",
    "\n",
    "# fname = '../data/projects/contiki-2.4/tools/tunslip.c'\n",
    "fname = '../data/projects/contiki-2.4/core/sys/timetable.c'\n",
    "\n",
    "apply_infer(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding target label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "\n",
    "def encode_multiclass(y):\n",
    "    \"\"\"encode multiclass target \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    encoded_y = encoder.transform(y)\n",
    "    with open('classes.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "    return encoded_y\n",
    "\n",
    "def decode_multiclass(encoded_y):\n",
    "    \"\"\"decode multiclass target \"\"\"\n",
    "    with open('classes.pkl', 'rb') as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    decoded_y = encoder.inverse_transform(encoded_y)\n",
    "    # decoded_y = [x[0] for x in decoded_y]\n",
    "    return decoded_y\n",
    "\n",
    "y = ['Benign', 'CWE-120', 'Benign', 'Benign', 'CWE-120', 'CWE-20', 'CWE-19']\n",
    "\n",
    "# target representation for binary classification\n",
    "y = [x if x=='Benign' else 'Vulnerable' for x in y]\n",
    "\n",
    "encoded_y = encode_multiclass(y)\n",
    "print(y)\n",
    "print(list(set(list(encoded_y))))\n",
    "print(decode_multiclass([0, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "np.unique(list(y), return_counts=True)\n",
    "pd.value_counts(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoded_y)/(2*np.bincount(encoded_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_multiclass_target(y):\n",
    "    \"\"\"encode multiclass target \"\"\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit_transform(y)\n",
    "    with open('../data/classes.pkl', 'wb') as f:\n",
    "        pickle.dump(encoder, f)\n",
    "\n",
    "def decode_multiclass_target(encoded_y):\n",
    "    \"\"\"decode multiclass target \"\"\"\n",
    "    with open('../data/classes.pkl', 'rb') as f:\n",
    "        encoder = pickle.load(f)\n",
    "\n",
    "    decoded_y = encoder.inverse_transform(encoded_y)\n",
    "    # decoded_y = [x[0] for x in decoded_y]\n",
    "    return decoded_y\n",
    "\n",
    "encode_multiclass_target(y)\n",
    "decode_multiclass_target([0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if tokenizer is working fine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import printable\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def tokenize_data(df, max_len):\n",
    "    \"\"\"Dataset tokenization\"\"\"\n",
    "    code_snippet_int_tokens = [\n",
    "        [printable.index(x) + 1 for x in code_snippet if x in printable]\n",
    "        for code_snippet in df.code]\n",
    "\n",
    "    # Pad the sequences (left padded with zeros)\n",
    "    # to the max length of the code snippet\n",
    "    # print(code_snippet_int_tokens)\n",
    "    X = pad_sequences(code_snippet_int_tokens, maxlen=max_len)\n",
    "    target = np.array(df.label)\n",
    "    print(f\"Shape of X: {X.shape}, Shape of y:{target.shape}\")\n",
    "    return X, target\n",
    "\n",
    "df = pd.read_csv('../data/TinyVul-v2-statement-multiclass.csv')\n",
    "tokenize_data(df.head(20), 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# function to perform OneHotEncoder to target\n",
    "def encode_multiclass(y):\n",
    "    \"\"\"Encode multiclass labels\"\"\"\n",
    "    # encode class values as integers\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(y)\n",
    "    print(f'Number of classes: {len(encoder.classes_)}')\n",
    "    encoded_Y = encoder.transform(y)\n",
    "    # convert integers to dummy variables (i.e. one hot encoded)\n",
    "    print(encoded_Y)\n",
    "    dummy_y = tf.keras.utils.to_categorical(encoded_Y)\n",
    "    return encoder, dummy_y\n",
    "\n",
    "def decode_multiclass(onehot_y, encoder):\n",
    "    \"\"\"decode multiclass target \"\"\"\n",
    "    # with open('data/classes.pkl', 'rb') as f:\n",
    "    #     encoder = pickle.load(f)\n",
    "\n",
    "    decoded_y = [encoder.inverse_transform([np.argmax(v)])[0] for v in onehot_y]\n",
    "    return decoded_y\n",
    "\n",
    "y = ['Benign', 'CWE-20', 'CWE-120', 'CWE-119', 'Benign', 'CWE-20']\n",
    "encoder, y = encode_multiclass(y)\n",
    "\n",
    "decode_multiclass(y, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ['Benign', 'CWE-20', 'CWE-120', 'CWE-119', 'Benign', 'CWE-20']\n",
    "dist = pd.Series(y).value_counts()\n",
    "print(len(dist))\n",
    "print(f'Distribution of targets: \\n{pd.Series(y).value_counts()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iotCode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f838046bc7a32c7f2766f1c532b5baddca8fb3dd7100bdb4fa26065ff771de8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
