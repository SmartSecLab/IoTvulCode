{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable-AI model on iDetect dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "from string import printable\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model, model_from_json, load_model\n",
    "from keras import regularizers\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda, Flatten\n",
    "from keras.layers import Input, ELU, LSTM, Embedding, Convolution2D, MaxPooling2D, \\\n",
    "BatchNormalization, Convolution1D, MaxPooling1D, concatenate\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Bidirectional, SimpleRNN\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.pipeline import TransformerMixin\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# <guru> Newly added API calls\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/iDetect_refine/DNN_Binary.csv', encoding='unicode_escape')\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_data(df):\n",
    "    \"\"\" Dataset tokenization\n",
    "    \"\"\"\n",
    "    code_snippet_int_tokens = [[printable.index(x) + 1 for x in code_snippet if x in printable] \n",
    "                            for code_snippet in df.code]\n",
    "    max_len = 150\n",
    "    # X = sequence.pad_sequences(code_snippet_int_tokens, maxlen=max_len) # original\n",
    "    X = pad_sequences(code_snippet_int_tokens, maxlen=max_len)\n",
    "    target = np.array (df.isMalicious)\n",
    "    print('Matrix dimensions of X: ', X.shape, 'Vector dimension of target: ', target.shape)\n",
    "    return X\n",
    "\n",
    "X = tokenize_data(df)\n",
    "#Split the data set into training and test data\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, target, test_size=0.30, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(df):\n",
    "    \"\"\" plot histplot of the code length \n",
    "    \"\"\"\n",
    "    len_arr = []\n",
    "    for i in range(len(df)):\n",
    "        len_arr.append(len(df.loc[i,'code']))\n",
    "    sns.histplot(len_arr)\n",
    "\n",
    "plot_hist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "\n",
    "seed = 101 # fix random seed for reproducibility\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "class TextsToSequences(Tokenizer, BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Sklearn transformer to convert texts to indices list \n",
    "    (e.g. [[\"the cute cat\"], [\"the dog\"]] -> [[1, 2, 3], [1, 4]])\"\"\"\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "    def fit(self, texts, y=None):\n",
    "        self.fit_on_texts(texts)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, texts, y=None):\n",
    "        return np.array(self.texts_to_sequences(texts))\n",
    "        \n",
    "sequencer = TextsToSequences(num_words=vocab_size)\n",
    "\n",
    "class Padder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Pad and crop uneven lists to the same length. \n",
    "    Only the end of lists longernthan the maxlen attribute are\n",
    "    kept, and lists shorter than maxlen are left-padded with zeros\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    maxlen: int\n",
    "        sizes of sequences after padding\n",
    "    max_index: int\n",
    "        maximum index known by the Padder, if a higher index is met during \n",
    "        transform it is transformed to a 0\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen=500):\n",
    "        self.maxlen = maxlen\n",
    "        self.max_index = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.max_index = pad_sequences(X, maxlen=self.maxlen).max()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X = pad_sequences(X, maxlen=self.maxlen)\n",
    "        X[X > self.max_index] = 0\n",
    "        return X\n",
    "\n",
    "padder = Padder(maxlen)\n",
    "\n",
    "def create_model(max_features):\n",
    "    \"\"\" Model creation function: returns a compiled LSTM\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 128))\n",
    "    model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 20000  # Max number of different word, i.e. model input dimension\n",
    "maxlen = 80  # Max number of words kept at the end of each text\n",
    "batch_size = 128\n",
    "max_features = vocab_size + 1\n",
    "\n",
    "\n",
    "# Use Keras Scikit-learn wrapper to instantiate a LSTM with all methods\n",
    "# required by Scikit-learn for the last step of a Pipeline\n",
    "sklearn_lstm = KerasClassifier(build_fn=create_model, epochs=2, batch_size=batch_size, \n",
    "                               max_features=max_features, verbose=1)\n",
    "\n",
    "# Build the Scikit-learn pipeline\n",
    "pipeline = make_pipeline(sequencer, padder, sklearn_lstm)\n",
    "\n",
    "print(pipeline)\n",
    "pipeline.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras; \n",
    "if int(keras.__version__.split('.')[0])>=2:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fafc45a6ddd39abcc5554869de579499dddcfa59fa9d77260344fc2dbb621709"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
